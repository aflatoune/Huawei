{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huawei Research France"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-24T15:37:24.212303Z",
     "start_time": "2021-06-24T15:37:24.148072Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rampwf as rw\n",
    "import datetime\n",
    "import time\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-24T15:37:24.336717Z",
     "start_time": "2021-06-24T15:37:24.289878Z"
    }
   },
   "outputs": [],
   "source": [
    "import extract\n",
    "import clean\n",
    "from extract import PrepareExtractor\n",
    "from clean import DataCleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(extract)\n",
    "importlib.reload(clean);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-24T14:45:53.736947Z",
     "start_time": "2021-06-24T14:45:53.706365Z"
    }
   },
   "outputs": [],
   "source": [
    "problem = rw.utils.assert_read_problem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajout des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-24T14:46:26.053569Z",
     "start_time": "2021-06-24T14:45:53.742341Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data\n",
      "Optical Dataset composed of\n",
      "46110 source samples\n",
      "50862 source background samples\n",
      "438 target labeled samples\n",
      "8202 target unlabeled samples\n",
      "29592 target background samples\n",
      " Optical Dataset labels composed of\n",
      "46110 labels of source samples\n",
      "438 labels of target samples\n",
      "\n",
      "Test data\n",
      "Optical Dataset composed of\n",
      "0 source samples\n",
      "0 source background samples\n",
      "17758 target labeled samples\n",
      "0 target unlabeled samples\n",
      "47275 target background samples\n",
      " Optical Dataset labels composed of\n",
      "0 labels of source samples\n",
      "17758 labels of target samples\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = problem.get_train_data()\n",
    "X_test, y_test = problem.get_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps clean :  32.41353893280029\n",
      "Temps prepare :  159.410786151886\n",
      "Temps clean :  0.3790011405944824\n",
      "Temps prepare :  1.2896771430969238\n"
     ]
    }
   ],
   "source": [
    "list_X = []\n",
    "\n",
    "for n, X_i in enumerate((X_train.source, X_train.target)):\n",
    "    data_cleaner = DataCleaner(drop_olt_recv=True)\n",
    "    X_ = data_cleaner.clean_data(X_i)\n",
    "    prep = PrepareExtractor()\n",
    "    X, y = prep.get_data(X_,\n",
    "                         col_names=data_cleaner.columns,\n",
    "                         size_sample=-1,\n",
    "                         resample={'unit': 'D', 'func': 'mean'},\n",
    "                         name=n)\n",
    "    list_X.append(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack(list_X)\n",
    "y = np.hstack([y_train.source, y_train.target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((46548, 64), (46548,))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "Il faut soumettre un extracteur de caractéristiques et un classifieur. La fonction `transform` de l'extracteur de caractéristiques est exécutée sur chaque donnée d'entrée (cible, source, bkg) et les tableaux résultants sont passés aux fonctions fit et predict du classifieur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-24T14:12:05.933472Z",
     "start_time": "2021-06-24T14:10:28.671521Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps clean :  39.49433183670044\n",
      "Temps prepare :  154.41123390197754\n",
      "(46110, 253)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "submissions/prepare_rf/feature_extractor.py:94: RuntimeWarning: Mean of empty slice\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps clean :  50.07652497291565\n",
      "Temps prepare :  173.56100821495056\n",
      "(50862, 253)\n",
      "Temps clean :  0.4492971897125244\n",
      "Temps prepare :  1.6324219703674316\n",
      "(438, 253)\n",
      "Temps clean :  8.725809812545776\n",
      "Temps prepare :  22.908020973205566\n",
      "(8202, 253)\n",
      "Temps clean :  29.15816903114319\n",
      "Temps prepare :  82.29805588722229\n",
      "(29592, 253)\n",
      "LGBMClassifier(n_estimators=500, random_state=44)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "submissions/prepare_rf/feature_extractor.py:94: RuntimeWarning: Mean of empty slice\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps clean :  15.754160165786743\n",
      "Temps prepare :  53.1059889793396\n",
      "(17758, 253)\n",
      "Temps clean :  50.34573221206665\n",
      "Temps prepare :  162.23746609687805\n",
      "(47275, 253)\n"
     ]
    }
   ],
   "source": [
    "trained_workflow = problem.workflow.train_submission('submissions/prepare_rf', X_train, y_train)\n",
    "y_test_pred = problem.workflow.test_submission(trained_workflow, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap    = problem.score_types[0]\n",
    "rec5  = problem.score_types[1]\n",
    "rec10 = problem.score_types[2]\n",
    "rec20 = problem.score_types[3]\n",
    "acc   = problem.score_types[4]\n",
    "auc   = problem.score_types[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ap test score    = 0.2249308868473206\n",
      "rec5 test score  = 0.1181342601776123\n",
      "rec10 test score = 0.21447253227233887\n",
      "rec20 test score = 0.3779424726963043\n",
      "acc test score   = 0.8457596576191012\n",
      "auc test score   = 0.6538917229814298\n"
     ]
    }
   ],
   "source": [
    "print('ap test score    = {}'.format(ap(y_test.target, y_test_pred[:,1])))\n",
    "print('rec5 test score  = {}'.format(rec5(y_test.target, y_test_pred[:,1])))\n",
    "print('rec10 test score = {}'.format(rec10(y_test.target, y_test_pred[:,1])))\n",
    "print('rec20 test score = {}'.format(rec20(y_test.target, y_test_pred[:,1])))\n",
    "print('acc test score   = {}'.format(acc(y_test.target, y_test_pred.argmax(axis=1))))\n",
    "print('auc test score   = {}'.format(auc(y_test.target, y_test_pred[:,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous utilisons une validation croisée dix fois (stratifiée lorsque les étiquettes sont disponibles) pour tous les ensembles de données. Dans chaque split, 20% des instances sont dans l'ensemble de validation, à l'exception des données cibles étiquetées qui servent principalement à la validation (pour obtenir une estimation non biaisée des scores de test, évalués entièrement sur des échantillons cibles étiquetés). Nous plaçons vingt points cibles étiquetés dans les splits d'entraînement. La raison en est que lorsque nous étendons nos services à large bande à la ville B, nous pouvons obtenir rapidement un petit ensemble de données étiquetées, mais nous aimerions déployer notre détecteur de défaillance sans attendre deux mois pour recueillir des données comparables à celles de la ville A.\n",
    "\n",
    "Le schéma de validation croisée (voir `problem.get_cv`) est implémenté dans la classe `TLShuffleSplit` de `external_imports.utils.cv.py`, si vous voulez y regarder de plus près.\n",
    "\n",
    "Vous êtes libre de jouer avec la coupure train/test et la validation croisée lors du développement de vos modèles mais sachez que nous utiliserons la même configuration sur le serveur officiel que celle du kit RAMP (sur un ensemble différent de quatre campagnes qui ne sera pas disponible pour vous).\n",
    "\n",
    "La cellule suivante passe par les mêmes étapes que le script d'évaluation officiel (`ramp-test`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = problem.get_cv(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "splits = problem.get_cv(X_train, y_train)\n",
    "\n",
    "y_test_preds = []\n",
    "for fold_i, (train_is, valid_is) in enumerate(splits):\n",
    "    trained_workflow = problem.workflow.train_submission(\n",
    "        'submissions/starting_kit', X_train, y_train, train_is)\n",
    "    X_fold_train = X_train.slice(train_is)\n",
    "    X_fold_valid = X_train.slice(valid_is)\n",
    "    \n",
    "    y_train_pred = problem.workflow.test_submission(trained_workflow, X_fold_train)\n",
    "    y_valid_pred = problem.workflow.test_submission(trained_workflow, X_fold_valid)\n",
    "    y_test_pred = problem.workflow.test_submission(trained_workflow, X_test)\n",
    "    print('-------------------------------------')\n",
    "    print('training ap on fold {} = {}'.format(\n",
    "        fold_i, ap(y_train.slice(train_is).target, y_train_pred[:,1])))\n",
    "    print('validation ap on fold {} = {}'.format(\n",
    "        fold_i, ap(y_train.slice(valid_is).target, y_valid_pred[:,1])))\n",
    "    print('test ap on fold {} = {}'.format(fold_i, ap(y_test.target, y_test_pred[:,1])))\n",
    "    \n",
    "    y_test_preds.append(y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous calculons à la fois le score moyen du test et le score de la mise en sac de vos dix modèles. Le classement officiel sera déterminé par le score de test mis en sac (sur des ensembles de données différents de ceux dont vous disposez). Votre score public sera le score de validation mis en sac (le calcul de la moyenne est [légèrement plus compliqué](https://github.com/paris-saclay-cds/ramp-workflow/blob/master/rampwf/utils/combine.py#L56) car nous devons nous occuper correctement des masques de validation croisée). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bagged_y_pred = np.array(y_test_preds).mean(axis=0)\n",
    "print('Mean ap score = {}'.format(\n",
    "    np.mean([ap(y_test.target, y_test_pred[:,1]) for y_test_pred in y_test_preds])))\n",
    "print('Bagged ap score = {}'.format(\n",
    "    ap(y_test.target, np.array([y_test_pred for y_test_pred in y_test_preds]).mean(axis=0)[:,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemple submissions\n",
    "\n",
    "Outre le kit de départ, nous vous proposons deux autres exemples de soumissions. L'extracteur de caractéristiques est le même dans les trois. `source_rf` est similaire au kit de départ, mais utilise des arbres plus nombreux et plus profonds, pour obtenir un meilleur score. `target_rf` est une autre soumission extrême qui utilise seulement l'instance d'entraînement de la cible (peu) étiquetée pour apprendre un classificateur. Il a une performance légèrement moins bonne que `source_rf` ce qui signifie que les données sources améliorent le classificateur même si les distributions sources et cibles sont différentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultats:\n",
    "|          | ap             | rec-5         | rec-10         | rec-20         | acc            |  auc           | \n",
    "|:---------|:--------------:|:-------------:|:--------------:|:--------------:|:--------------:|:--------------:|   \n",
    "|source_rf | 0.191 ± 0.0026 | 0.073 ± 0.002 | 0.176 ± 0.0032 | 0.357 ± 0.0075 | 0.84 ± 0.0014  | 0.637 ± 0.0063 | \n",
    "|target_rf | 0.163 ± 0.0218 | 0.067 ± 0.0182| 0.138 ± 0.0339 | 0.272 ± 0.0537 | 0.813 ± 0.036  | 0.591 ± 0.0399 | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La grande question de l'apprentissage par transfert à résoudre est la suivante : **Comment combiner les données cibles à faible biais et à haute variance avec les données sources à faible biais et à haute variance**. D'autres questions auxquelles nous nous attendons à voir des réponses :\n",
    "\n",
    "1. Peut-on faire un meilleur prétraitement (amputation des données manquantes, utilisation du temps d'une manière plus intelligente) dans l'extracteur de caractéristiques ?\n",
    "2. Normalement, les données d'arrière-plan (bonnes instances) ne participent pas au scoring, mais elles peuvent informer le classifieur du changement de distribution. Comment utiliser au mieux cette information ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local testing (before submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;178m\u001b[1mTesting Optical access network failure prediction\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1mReading train and test files from ./data/ ...\u001b[0m\n",
      "Train data\n",
      "Optical Dataset composed of\n",
      "46110 source samples\n",
      "50862 source background samples\n",
      "438 target labeled samples\n",
      "8202 target unlabeled samples\n",
      "29592 target background samples\n",
      " Optical Dataset labels composed of\n",
      "46110 labels of source samples\n",
      "438 labels of target samples\n",
      "\n",
      "Test data\n",
      "Optical Dataset composed of\n",
      "0 source samples\n",
      "0 source background samples\n",
      "17758 target labeled samples\n",
      "0 target unlabeled samples\n",
      "47275 target background samples\n",
      " Optical Dataset labels composed of\n",
      "0 labels of source samples\n",
      "17758 labels of target samples\n",
      "\n",
      "Train data\n"
     ]
    }
   ],
   "source": [
    "!ramp-test --submission prepare_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to have a local leaderboard, use the `--save-output` option when running `ramp-test`, then try `ramp-show leaderboard` with different options. For example:\n",
    "```\n",
    "ramp-show leaderboard --mean --metric \"['ap','auc']\" --step \"['valid','test']\" --precision 3\n",
    "```\n",
    "and\n",
    "```\n",
    "ramp-show leaderboard --bagged --metric \"['auc']\"\n",
    "```\n",
    "\n",
    "RAMP also has an experimental hyperopt feature, with random grid search implemented. If you want to use it, type\n",
    "```\n",
    "ramp-hyperopt --help\n",
    "```\n",
    "and check out the example submission [here](https://github.com/ramp-kits/titanic/tree/hyperopt/submissions/starting_kit_h)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
